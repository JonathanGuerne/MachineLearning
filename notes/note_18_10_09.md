# Machine learning 

## Regréssion linéaire 

Approche de machine learning suppervisé.

la régression linéaire cherche à faire passer une droite au centre des données. Grâce à 
l'équation de cette droite on va pouvoir définir la valeur Y^ d'un X. 

np.polyfit(x,y,deg=) peut être utilisé pour trouvé le polynomes qui suit le mieux possible les données. voir *p.11*

### Descente de gradient 

Génération de N droites aléatoires => On sélectionne la meilleure avec l'ensemble de test.

Pour définir quelle droite est la meilleur on utilise un fonction de coùt

$J(\Theta)=\frac{1}{2N}\sum^{N}_{n=1}(h_\Theta(x_n) - y_n)^2$

l'erreur est donc calculé sur un couple de valeur ($\theta_0$,$\theta_1$) qui sont les facteurs de la droites.

$h(x)=\theta_0+\theta_1*x_1$

1 epoch = 1 vistite complète du triaing set. C'est aussi une mise à jour complète des theta.

pour update les valeurs de $\Theta$ on utilise des dérivées. 

$\theta_i \leftarrow \theta_i-\alpha\frac{\partial J(\Theta)}{\partial\theta_i}*x_{n,i}$

où $\alpha$ est le learning rate, un hyper paramètre qu'il faut spécifier.

On doit arrêter la descente quand on ne progresse plus. Pour cela on utlise un $\epsilon$ pour définir la marge à laquelle on arrête la descente. 

- régression par batch - tts les données
- régression stochastique - une donnée aléatoire
- régression par mini-batch - n données aléatoires 

Ce qui est très intéressant c'est que l'implémenattion retse la même avec un espace d'entrée à D dimensions.