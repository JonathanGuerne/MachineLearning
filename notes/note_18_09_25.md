# Machine learning

une classification lie une entrée X à un ensemble fini discret de sortie Y. les outputs sont les différentes label correspondant aux catégories qu'on cherche.

## K-nn : K plus proches voisins 
on va regarder les points dans le voisinages. on définit son état avec ses voisins (on utilise un vote majoritaire). On repère les K plus proches voisins et on prend le label le plus représenté chez les voisins. Le paramètre K doit être choisi par test. On doit également définir quel métric on utilise. 

training-set non balancé : les éléments présentent dans l'ensemble de test ne sont pas uniformément représentés

### 1 nn
pour K-nn avec k=1. On prédit le même label que l'instance la plus proche dans l'ensemble d'entraînement.

apprentissage : O(1)
prédiction : O(n) MAIS on est linéaire par rapport à l'ensemble de d'entraînement => ça prend du temps.

c'est embêtant parceque on voudrait un système rapide à la prédiction et qui n'aurait pas besoin d'être rapide en entraînement.

### 1-nn vs k-nn
plus grand K = plus de confiance dans la prédiction (moins de bruits).
k-nn peut fournir une probabilité des informations.
des trops grandes valeurs de K sont néanmoins mauvaise, on va commencer à créer des "clusters" qui n'existe pas réellement. 

dans le cas d'un "tie" avec un knn on a un problème. C'est à dire si le nombre de vote est égale pour 2 classes. alors on peut par exemple utiliser la distance pour appliquer des pondérations aux différents votes.

### comment choisir les hyper-paramètres ?
choisir la valeur de K, choisir la distance métrique,stratégie en cas d'égalité,...

#### idea 1
choisir la valeur qui marche le mieux dans notre ensemble d'entraînement.
mauvais : k=1 marche toujours à la perfection dans les données d'entraînement.
#### idea 2
découper les données en entraînement et test. chercher les hyper-paramètres qui marche le mieux sur les données de test
#### idea 3
découper les données en entraînement, validation, test. choisir les hyperparamètres sur validation et évaluer sur test.
#### idea 4
comme l'idée 3 mais on prend plusieurs échantillons de validations dans les données de test. On fait la moyenne des résultats obtenus (voir page 20)

## cas pratique classification d'image
